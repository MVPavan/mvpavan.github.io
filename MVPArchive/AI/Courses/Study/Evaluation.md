 [LLM Evaluation Guidebook](https://github.com/huggingface/evaluation-guidebook):

**1. Automatic Benchmarks:**

- Automated evaluations utilize predefined datasets and metrics to assess LLM performance without human intervention.
- 

**2. Human Evaluation:**

- _Basics:_ Involves human reviewers assessing LLM outputs based on criteria like relevance, coherence, and accuracy.
- _Using Human Annotators:_ Guidelines on selecting and training annotators to ensure consistent and unbiased evaluations.
- _Tips and Tricks:_ Recommendations include clear annotation guidelines, pilot testing, and regular calibration sessions among annotators.

**3. LLM-as-a-Judge:**

- _Basics:_ Leveraging LLMs to evaluate outputs from other models or systems.
- _Getting a Judge-LLM:_ Steps to select or fine-tune an LLM specifically for evaluative tasks.
- _Designing Your Evaluation Prompt:_ Crafting prompts that elicit accurate and consistent evaluations from the judge-LLM.
- _Evaluating Your Evaluator:_ Methods to assess the reliability and validity of the judge-LLM's evaluations.
- _What About Reward Models:_ Discussion on using reward models that predict scores based on human preferences to guide LLM evaluations.
- _Tips and Tricks:_ Addressing biases, ensuring consistency, and implementing self-consistency checks in LLM-based evaluations.



