Material from https://hanlab.mit.edu/courses/2023-fall-65940 lectures 17,18.

Types of distributed training:
- Data Parallel: Replicate model across gpu and distribute data across each gpu.
	- DP
	- DDP
	- FSDP
	- Deepspeed:
		- Zero - 1,2,3
- Pipeline Parallel:
- Tensor Parallel:


## Data Parallel:


![[Pasted image 20240703171020.png]]
- Replicate 

![[Pasted image 20240703171554.png]]