  
## Overview
`parallelize` is a high-level function in `std.algorithm` that automatically distributes work across multiple CPU cores for parallel execution. It is designed for CPU-intensive tasks where operations are independent.
### Key Features
* Â  **True Parallelism**: Unlike Python threading (which is limited by the GIL), Mojo's `parallelize` uses all available CPU cores.
* Â  **Low Overhead**: Uses a thread pool to minimize the cost of spawning threads.
* Â  **Shared Memory**: Parallel workers can access shared memory directly (safe with Mojo's ownership system), avoiding the high cost of inter-process communication (IPC) seen in Python's `multiprocessing`.
## Key Differences: `map` vs `parallelize`

| Feature       | `map`                      | `parallelize`             |
| ------------- | -------------------------- | ------------------------- |
| **Execution** | Sequential (one at a time) | Parallel (multiple cores) |
| **Use Case**  | Simple iteration           | CPU-intensive tasks       |
| **Overhead**  | Minimal                    | Thread creation overhead  |
| **Best For**  | Small/fast operations      | Large computations        |
## Function Signatures

```mojo
from algorithm import parallelize
# Basic version - auto-detects CPU cores
# func signature: fn(idx: Int)
fn parallelize[func: fn(Int) capturing [origins] -> None](num_work_items: Int)
# With explicit worker count
fn parallelize[func: fn(Int) capturing [origins] -> None](
Â  Â  num_work_items: Int,
Â  Â  num_workers: Int
)
```
## How It Works
1. Â **Work Distribution**: Divides `num_work_items` into chunks.
2. Â **Thread Pool**: Uses a pool of worker threads (defaulting to the number of logical CPU cores).
3. Â **Parallel Execution**: Each worker processes its assigned range of indices.
4. Â **Synchronization**: The function blocks until all workers complete.
## Usage Examples
### 1. Basic Element-wise Processing
A common pattern is processing an array of data.
```mojo
from algorithm import parallelize
var data = List[Int](capacity=1000)
# ... initialize data ... 
@parameter
fn worker(idx: Int):
Â  Â  # Each worker accesses a unique index, ensuring thread safety
Â  Â  data[idx] = data[idx] * 2
parallelize[worker](1000)
```
### 2. Reductions (Safe Pattern)
To safely aggregate results (e.g., sum), give each worker its own storage slot to avoid race conditions.
```mojo
var num_workers = 4
var partial_sums = List[Int](capacity=num_workers)
for _ in range(num_workers):
Â  Â  partial_sums.append(0)
@parameter
fn worker(worker_id: Int):
Â  Â  # Perform computation and write to specific slot
Â  Â  partial_sums[worker_id] = compute_heavy_sum(worker_id)
# Parallelize with explicit worker count
parallelize[worker](num_workers, num_workers)
# Combine results sequentially
var total = 0
for i in range(num_workers):
Â  Â  total += partial_sums[i]
```
## When to Use

> [!success] Use `parallelize` when:
> * Â  Processing large datasets (1000+ items).
> * Â  Each operation is CPU-intensive (>1Î¼s per item).
> * Â  Operations are independent (no data dependencies between indices).
> * Â  Computation time significantly exceeds thread management overhead.

> [!failure] Avoid `parallelize` when:
> * Â  Small datasets or very fast operations (use `map` or simple loops).
> * Â  Operations have complex inter-dependencies.
> * Â  The task is purely I/O bound (waiting for network/disk) - though it may still work, concurrency (Async) might be more appropriate.
## Safety & Best Practices

> [!important] Best Practices
> 1. Â **Avoid Race Conditions**: Never write to the same memory location from multiple workers without synchronization.
> Â  Â  * Â  *Bad*: `counter += 1` inside worker.
> Â  Â  * Â  *Good*: `partial_counts[idx] = count` inside worker.
> 1. Â **Origin Tracking**: Mojo automatically tracks captured variables. Ensure captured mutable variables are not aliased in unsafe ways.
> 2. Â **Chunk Size**: If your work items are tiny, consider processing chunks of items inside the worker function to reduce overhead.
> 3. Â **Memory Layout**: Use contiguous memory (List, arrays) to maximize cache efficiency across cores.


## Comparison with Python
Mojo's `parallelize` is most similar to Python's **`multiprocessing.Pool`**, but significantly faster and easier to use.

| Feature              | Mojo `parallelize` | Python `multiprocessing` | Python `threading`   |
| -------------------- | ------------------ | ------------------------ | -------------------- |
| **True Parallelism** | âœ… Yes              | âœ… Yes                    | âŒ No (GIL)           |
| **Best For**         | CPU-bound tasks    | CPU-bound tasks          | I/O-bound tasks      |
| **Shared Memory**    | âœ… Direct Access    | âŒ IPC / Manager needed   | âœ… Direct Access      |
| **Overhead**         | Low (Thread Pool)  | High (Process creation)  | Low                  |
| **Performance**      | ğŸš€ Native Speed    | ğŸŒ Slower (Pickling/IPC) | ğŸ¢ Single-core limit |
### Visual Comparison

**Python `threading` (GIL Limitation):**
```
Thread 1: [====GIL====]     [====GIL====]
Thread 2:     [====GIL====]     [====GIL====]
Thread 3:         [====GIL====]     [====GIL====]
          â†‘ Only ONE thread executes Python code at a time
```

 **Python `multiprocessing`:**
```
Process 1: [=============] [=============]
Process 2: [=============] [=============]
Process 3: [=============] [=============]
           â†‘ True parallelism, but high overhead
```

**MojoÂ `parallelize`:**
```
Worker 1: [=============] [=============]
Worker 2: [=============] [=============]
Worker 3: [=============] [=============]
          â†‘ True parallelism with shared memory!
```

**PythonÂ `asyncio`:**
```
Single Thread:
Task 1: [==]    [==]    [==]
Task 2:    [==]    [==]    [==]
Task 3:       [==]    [==]    [==]
        â†‘ Cooperative multitasking (not parallel)
```


### Summary Table

| Feature       | `parallelize` | `multiprocessing` | `threading`   | `asyncio`         |
| ------------- | ------------- | ----------------- | ------------- | ----------------- |
| Parallelism   | âœ… True        | âœ… True            | âŒ GIL-limited | âŒ Concurrent only |
| CPU-Bound     | â­â­â­â­â­         | â­â­â­               | â­             | â­                 |
| I/O-Bound     | â­â­â­           | â­â­                | â­â­â­â­          | â­â­â­â­â­             |
| Shared Memory | âœ… Direct      | âŒ IPC needed      | âœ… Direct      | âœ… Direct          |
| Overhead      | Low           | High              | Very Low      | Very Low          |
| Ease of Use   | â­â­â­â­â­         | â­â­â­               | â­â­â­â­          | â­â­                |

> [!info] The Bottom LineÂ **Mojo'sÂ `parallelize`Â is like Python'sÂ `multiprocessing.Pool`Â but:**
> 
> - ğŸš€Â **10-50x faster**Â (no process overhead)
> - ğŸ’¾Â **Direct shared memory access**Â (no IPC)
> - ğŸ¯Â **Simpler API**Â (no pickling, no process management)
> - âš¡Â **No GIL**Â (true parallelism by default)
> 
> It gives you theÂ **performance of C++ threads**Â with theÂ **simplicity of Python's API**!

