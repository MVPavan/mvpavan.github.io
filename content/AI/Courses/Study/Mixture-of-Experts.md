[Model Merging, Mixtures of Experts, and Towards Smaller LLMs](https://magazine.sebastianraschka.com/p/research-papers-in-january-2024?open=false#%C2%A7mixtral-of-experts)

[Home | Substack](https://substack.com/home/post/p-148217245)

![[Pasted-image-20250108161018.png]]


![[Pasted-image-20250131084119.png]]

![[Pasted-image-20250131084509.png]]

![[Pasted-image-20250211114836.png]]

![[Pasted-image-20250131084526.png]]

![[Pasted-image-20250131084538.png]]


![[Pasted-image-20250211114937.png]]



### Load Balancing:

Â to prevent overfitting on the same experts

![[Pasted-image-20250211115226.png]]


## Auxiliary Loss

![[Pasted-image-20250211135634.png]]





