<!doctype html>




<html
    dir="ltr"
    lang="en"
    
>
    <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#ffffff">
    <link rel="stylesheet" href="/assets/css/app.css">
    <link
        rel="shortcut icon"
        type="image/png"
        
            href="/favicon.png"
        
    >
    <script defer src="https://unpkg.com/alpinejs@3.9.0/dist/cdn.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>MVPavan’s Notes | Welcome to my GitHub Pages site.</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="MVPavan’s Notes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to my GitHub Pages site." />
<meta property="og:description" content="Welcome to my GitHub Pages site." />
<link rel="canonical" href="http://localhost:4000/MVPArchive/AI/Courses/EfficientML/Quantization/" />
<meta property="og:url" content="http://localhost:4000/MVPArchive/AI/Courses/EfficientML/Quantization/" />
<meta property="og:site_name" content="MVPavan’s Notes" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="MVPavan’s Notes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Welcome to my GitHub Pages site.","headline":"MVPavan’s Notes","url":"http://localhost:4000/MVPArchive/AI/Courses/EfficientML/Quantization/"}</script>
<!-- End Jekyll SEO tag -->

    <!-- head scripts -->
</head>

    <body>
        
            <div class="">
    <script
        src="https://cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js"
    ></script>

    <div class="container py-6 px-4" id="cookieBanner">
        <div class="columns">
            <div class="column is-fullwidth ">
                <div class="content">
                    <p>We use cookies on this site to enhance your user experience</p>
                    <p>
                        By clicking the Accept button, you agree to us doing so.
                        <a href="/cookie-policy/">More info on our cookie policy</a>
                    </p>
                </div>

                <div class="buttons">
                    <button class="button is-primary" onclick="acceptCookies()">Accept all cookies</button>
                    <button class="button is-primary" onclick="rejectCookies()">Reject all cookies</button>
                </div>
            </div>
        </div>
    </div>

    <script>
        function acceptCookies() {
            
            Cookies.set('showCookieBanner', 'false', { expires: 7, path: '/' });
            Cookies.set('cookiesAccepted', 'true', {expires: 7, path: '/'});
            toggleCookieBanner();
        }

        function rejectCookies() {
            Cookies.set('showCookieBanner', 'false', { expires: 7, path: '/' });
            toggleCookieBanner();
        }

        function toggleCookieBanner() {
            var showBanner = Cookies.get('showCookieBanner');
            var banner = document.getElementById('cookieBanner');

            if (showBanner === 'false') {
                banner.setAttribute('style', 'display: none;');
            } else {
                banner.setAttribute('style', 'display: block;');
            }
        }

        toggleCookieBanner();
    </script>
</div>

        
        <nav
    class="navbar is-primary "
    x-data="{ openNav: false }"
>
    <div class="container">
        <div class="navbar-brand">
            <a href="/" class="navbar-item">
                MVPavan's Notes
            </a>
            <a
                role="button"
                class="navbar-burger burger"
                aria-label="menu"
                aria-expanded="false"
                data-target="navMenu"
                :class="{ 'is-active': openNav }"
                x-on:click="openNav = !openNav"
            >
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu" :class="{ 'is-active': openNav }">
            <div class="navbar-start">
                <a href="/" class="navbar-item ">Home</a>
                
                    
                        
                            <a
                                href="/docs/"
                                class="navbar-item "
                            >Docs</a>
                        
                    
                
            </div>

            <div class="navbar-end">
                
            </div>
        </div>
    </div>
</nav>

        
            <section
    class="hero  is-medium  is-bold is-primary"
    
>
    <div class="hero-body ">
        <div class="container">
            <h1 class="title is-2"></h1>
            <p class="subtitle is-3"></p>
            
        </div>
    </div>
</section>

        
        

        <section class="section">
            <div class="container">
                <div class="columns is-multiline">
                    
                    <div class="column is-8">
                        

                        

                        

                        

                        
<div class="content">
    
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#Introduction</td>
          <td>Introduction]]</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#Types of Quantization</td>
          <td>Types of Quantization]]</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>[[#Types of Quantization#K-Means-based Weight Quantization</td>
              <td>K-Means-based Weight Quantization]]</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>[[#Types of Quantization#Linear Quantization</td>
              <td>Linear Quantization]]</td>
            </tr>
          </tbody>
        </table>
        <ul>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>[[#Linear Quantization#Quantization Granularities</td>
                  <td>Quantization Granularities]]</td>
                </tr>
              </tbody>
            </table>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#Quantization in Training</td>
          <td>Quantization in Training]]</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>[[#Quantization in Training#Quantization aware Training (QAT)</td>
              <td>Quantization aware Training (QAT)]]</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#STOA Quantization Techniques</td>
          <td>STOA Quantization Techniques]]</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>[[#STOA Quantization Techniques#LLM.int8</td>
              <td>LLM.int8]]</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>[[#STOA Quantization Techniques#GPTQ</td>
              <td>GPTQ]]</td>
            </tr>
          </tbody>
        </table>
        <ul>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>[[#GPTQ#Optimal Brain Quantizer (OBQ)</td>
                  <td>Optimal Brain Quantizer (OBQ)]]</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>[[#GPTQ#GPTQ Algorithm</td>
                  <td>GPTQ Algorithm]]</td>
                </tr>
              </tbody>
            </table>
          </li>
        </ul>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>[[#STOA Quantization Techniques#SmoothQuant</td>
              <td>SmoothQuant]]</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>[[#STOA Quantization Techniques#Activation Aware Quantization</td>
              <td>Activation Aware Quantization]]</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>[[#STOA Quantization Techniques#QLoRA NF4</td>
              <td>QLoRA NF4]]</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<ul>
  <li>INT8 (S1-B7) = 1 bytes (for INT there is not exponent and mantissa only bits)</li>
  <li>FP32 (1 sign, 8 exponent, 23 mantissa - S1-E8-M23) - requires 32 bits to store = 4x8 bits = 4 bytes</li>
  <li>FP16 (S1-E5-M10) = 2 bytes</li>
  <li>BF16 (S1-E8-M7) = 2 bytes</li>
  <li>FP8 (S1-E4-M3) = 1 bytes</li>
  <li>FP8 (S1-E5-M2) = 1 bytes</li>
  <li>FP4 S1E3/S1E2M1 = 0.5 bytes</li>
  <li>Subnormal values -  E = 0</li>
  <li>Normal Values - E != 0</li>
</ul>

<blockquote>
  <p>If any kernels are not implemented in fp16 try in bf16 as it is compatible with fp32 for same E</p>
</blockquote>

<p>| Data Type    | Size (bits) | Sign Bits | Exponent | Mantissa | Precision | Range  |
| ———— | ———– | ——— | ——– | ——– | ——— | —— |
| <strong>Float32</strong>  | 32          | 1         | 8        | 23       | Best      | ~10^38 |
| <strong>Float16</strong>  | 16          | 1         | 5        | 10       | Better    | ~10^4  |
| <strong>BFloat16</strong> | 16          | 1         | 8        | 7        | Good      | ~10^38 |
| FP8          | 8           | 1         | 4/5      | 3/2      | Avg       |        |
| <strong>Int8</strong>     | 8           | 1         | N/A      | N/A      | Avg       | ~2^7   |
<img src="attachments/Pasted%20image%2020240705132049.png" alt="" /></p>
<h2 id="types-of-quantization">Types of Quantization</h2>

<p>Quantization can happen in two ways:</p>
<ul>
  <li>Activation Quantization (Calibration)
    <ul>
      <li>Range of activations varies with input</li>
      <li>Use sample input, infer and calculate range of activations for linear quantization.</li>
    </ul>
  </li>
  <li>Weight Quantization
    <ol>
      <li>K-Means-based Weight Quantization</li>
      <li>Linear Quantization
        <ol>
          <li>AWQ: Activation aware weight quantization</li>
          <li>GPTQ: GPT Quantized</li>
          <li>BNB: Bits and Bytes Quantized</li>
        </ol>
      </li>
    </ol>
  </li>
</ul>

<p><img src="attachments/Pasted%20image%2020240705091622.png" alt="" /></p>

<h3 id="k-means-based-weight-quantization">K-Means-based Weight Quantization</h3>

<ul>
  <li>Cluster weights into M clusters using any clustering algorithm.</li>
  <li>QAT:
    <ul>
      <li>Obtain gradients</li>
      <li>Cluster gradients</li>
      <li>And apply to centroids as shown below</li>
    </ul>
  </li>
</ul>

<p><img src="attachments/Pasted%20image%2020240705193321.png" alt="" /></p>

<p><img src="attachments/Pasted%20image%2020240704184537.png" alt="" /></p>

<h3 id="linear-quantization">Linear Quantization</h3>
<ul>
  <li>Symmetric</li>
  <li>Asymmetric</li>
</ul>

<p>An affine mapping of integers to real numbers ==r = S(q-Z)</p>

<p><img src="attachments/Pasted%20image%2020240704184804.png" alt="" /></p>

<p>Full Connected Layer and Conv Layer:
<img src="attachments/Pasted%20image%2020240704185026.png" alt="" /></p>

<p>Symmetric (SQ) vs Asymmetric (ASQ):
<img src="attachments/Pasted%20image%2020240704185237.png" alt="" /><img src="attachments/Pasted%20image%2020240704185253.png" alt="" /></p>
<ul>
  <li>In practice symmetric Q is used for 8 bit Q and asymmetric for 2,4 etc..</li>
  <li>Symmetric Q is simple but can have not so tight range which effects the Q precision and Q error.</li>
</ul>

<h4 id="quantization-granularities">Quantization Granularities</h4>
<ul>
  <li>Tensor - S,Z are calculated per tensor</li>
  <li>Channel/Vector - S,Z are calculated per channel</li>
  <li>Group - S,Z are calculated per group, group size are usually n=32,64,128.. elements
    <ul>
      <li>Lets say n=32 and using 4-bit SQ, resulting tensor is actually Qed to 4.5 bit</li>
      <li>4-bit (every element is stored in 4 bit) + 0.5 bit (scale in 16 bits for every 32 bits (group size n), 16/32)</li>
    </ul>
  </li>
</ul>

<h2 id="quantization-in-training">Quantization in Training</h2>
<ul>
  <li>Post Training Quantization (PTQ) - All methods above</li>
  <li>Quantization aware Training (QAT)</li>
</ul>

<h3 id="quantization-aware-training-qat">Quantization aware Training (QAT)</h3>
<ul>
  <li>Quantize:
    <ul>
      <li>Quantize weight and hold both quantized (BF16) and unquantized (FP32) weights.</li>
    </ul>
  </li>
  <li>Forward Pass:
    <ul>
      <li>Use Quantized version of model for inference (BF16)</li>
    </ul>
  </li>
  <li>Back Prop:
    <ul>
      <li>Use original unquantized version of model weights (FP32)</li>
    </ul>
  </li>
</ul>

<h2 id="stoa-quantization-techniques">STOA Quantization Techniques</h2>
<ul>
  <li>LLM.int8</li>
  <li>GPTQ (OBQ)</li>
  <li>AWQ</li>
  <li>QLoRA - NF4</li>
</ul>

<h3 id="llmint8">LLM.int8</h3>

<p><img src="attachments/Mixed-int8.gif" alt="" />
LLM.int8() seeks to complete the matrix multiplication computation in three steps:</p>

<ol>
  <li>From the input hidden states, extract the outliers (i.e. values that are larger than a certain threshold) by column.</li>
  <li>Perform the matrix multiplication of the outliers in FP16 and the non-outliers in int8 after quantizing.</li>
  <li>Dequantize the non-outlier results and add both outlier and non-outlier results together to receive the full result in FP16.</li>
</ol>

<h3 id="gptq">GPTQ</h3>

<p>GPTQ is a one-shot weight quantization method based on approximate second-order information, that is both highly accurate and highly-efficient. GPTQ is inspired from OBQ.</p>
<h4 id="optimal-brain-quantizer-obq">Optimal Brain Quantizer (OBQ)</h4>
<ul>
  <li><strong>Objective:</strong> Minimize performance degradation by quantizing weights W<sup>^</sup>​ such that the outputs W<sup>^</sup>X​ closely match the original outputs WX</li>
  <li><strong>Quantization Process:</strong> Quantize the easiest weight first W<sub>q</sub>, then adjust remaining non-quantized weights using $\delta$<sub>F</sub> to compensate for the quantization error using the Hessian matrix.</li>
  <li><strong>Outlier Handling:</strong> Quantize outlier weights immediately to prevent large quantization errors.</li>
  <li><strong>Hessian Matrix Adjustment:</strong> For next iteration update the Hessian matrix by removing the row and column associated with the quantized weight using Gaussian elimination to avoid redundant computations.</li>
</ul>

<p><img src="attachments/Pasted%20image%2020240705172944.png" alt="" /></p>

<h4 id="gptq-algorithm">GPTQ Algorithm</h4>
<ul>
  <li><strong>Objective:</strong> Reduce the model size while minimizing MSE same as OBQ.</li>
  <li><strong>Calibration Dataset:</strong> Use a representative calibration dataset to perform inference on the quantized model, which helps in refining the quantization quality.</li>
  <li><strong>Quantization Scheme:</strong> Use a hybrid quantization scheme where model weights are quantized as int4, and activations are kept in float16. During inference, weights are dynamically dequantized to float16, and actual computations are performed in float16.</li>
</ul>

<p>Algorithm:</p>
<ol>
  <li>The GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse (a matrix that helps decide how to adjust the weights)</li>
  <li>It then runs in loops, handling batches of columns at a time.</li>
  <li>For each column in a batch, it quantizes the weights, calculates the error, and updates the weights in the block accordingly.</li>
  <li>After processing the batch, it updates all remaining weights based on the block’s errors.</li>
</ol>

<p><img src="attachments/Pasted%20image%2020240705173101.png" alt="" /></p>

<h3 id="smoothquant">SmoothQuant</h3>
<ul>
  <li>Quantizing weights is very easy</li>
  <li>Quantizing activations is input dependent and is prone to more outliers</li>
  <li>Migrate the difficulty (outliers) from activations to weights.</li>
</ul>

<p><img src="attachments/Pasted%20image%2020240705175717.png" alt="" /></p>

<h3 id="activation-aware-quantization">Activation Aware Quantization</h3>

<ol>
  <li><strong>Collect Activation Statistics</strong>: During this calibration phase, a subset of the data is used to collect statistics on the activations produced by the model. This involves running the model on this data and recording the range of values and the distribution of activations.</li>
  <li><strong>Search Weight Quantization Parameters</strong>: Weights are quantized by taking the activation statistics into account (scale). Concretely, we perform a space search for quantization parameters (e.g., scales and zero points), to minimize the distortions incurred by quantization on output activations. As a result, the quantized weights can be accurately represented with fewer bits.</li>
  <li><strong>Quantize</strong> : With the quantization parameters in place, the model weights are quantized using a reduced number of bits.</li>
</ol>

<p><img src="attachments/Pasted%20image%2020240705192348.png" alt="" /></p>
<ul>
  <li>Just keeping 1% of salient weight improves PPL(perplexity) from 43.2 to 13.0</li>
  <li>(c) Weights are scales based on activation statistics before Quantization.
    <h3 id="qlora-nf4">QLoRA NF4</h3>
  </li>
  <li>Here 4 bits are used to represent quantization level rather than actual values. So 4 bits can represent 16 levels.</li>
  <li><strong>Range Selection:</strong> For NF4, the range is chosen to cover most of the weight values typically found in language models. Usually the range is [-1, 1] as layers are normalized.</li>
  <li><strong>Mapping:</strong> Determine the 16 Equally/logarithmically spaced quantization levels within this range. For equal spaced example levels might be [-1, -0.8667, -0.733, …,0.6, 0.733, 0.8667, 1].</li>
  <li><strong>Mapping Weight:</strong> Lets say our weight is 0.5678 find the closest NF4 value to it which is 0.6.</li>
  <li><strong>Storage:</strong> Now instead of storing 0.6 like in linear mapping Q, store the level of 0.6 which is 13 as a 4-bit value.</li>
  <li><strong>De-Q</strong>: Convert 13 level to 0.6 and use it, now Q err is 0.6-0.5678 = 0.0322</li>
</ul>


</div>

                    </div>
                    
                        <div class="column is-4-desktop is-4-tablet">
                            <p class="title is-4">Latest Posts</p>

<div class="columns is-multiline">
    
</div>

                        </div>
                    
                </div>
            </div>
        </section>
        
            <footer class="footer">
    <div class="container">
        
            <div class="columns is-multiline">
                
            </div>
        

        <div class="content is-small has-text-centered">
            <p class="">Theme built by <a href="https://www.csrhymes.com">C.S. Rhymes</a></p>
        </div>
    </div>
</footer>

        
        <script src="/assets/js/app.js" type="text/javascript"></script><!-- footer scripts -->
</body>
</html>
