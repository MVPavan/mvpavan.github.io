<!doctype html>




<html
    dir="ltr"
    lang="en"
    
>
    <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#ffffff">
    <link rel="stylesheet" href="/assets/css/app.css">
    <link
        rel="shortcut icon"
        type="image/png"
        
            href="/favicon.png"
        
    >
    <script defer src="https://unpkg.com/alpinejs@3.9.0/dist/cdn.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>MVPavan’s Notes | Welcome to my GitHub Pages site.</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="MVPavan’s Notes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to my GitHub Pages site." />
<meta property="og:description" content="Welcome to my GitHub Pages site." />
<link rel="canonical" href="http://localhost:4000/MVPArchive/AI/Courses/EfficientML/Distributed%20Training/" />
<meta property="og:url" content="http://localhost:4000/MVPArchive/AI/Courses/EfficientML/Distributed%20Training/" />
<meta property="og:site_name" content="MVPavan’s Notes" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="MVPavan’s Notes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Welcome to my GitHub Pages site.","headline":"MVPavan’s Notes","url":"http://localhost:4000/MVPArchive/AI/Courses/EfficientML/Distributed%20Training/"}</script>
<!-- End Jekyll SEO tag -->

    <!-- head scripts -->
</head>

    <body>
        
            <div class="">
    <script
        src="https://cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js"
    ></script>

    <div class="container py-6 px-4" id="cookieBanner">
        <div class="columns">
            <div class="column is-fullwidth ">
                <div class="content">
                    <p>We use cookies on this site to enhance your user experience</p>
                    <p>
                        By clicking the Accept button, you agree to us doing so.
                        <a href="/cookie-policy/">More info on our cookie policy</a>
                    </p>
                </div>

                <div class="buttons">
                    <button class="button is-primary" onclick="acceptCookies()">Accept all cookies</button>
                    <button class="button is-primary" onclick="rejectCookies()">Reject all cookies</button>
                </div>
            </div>
        </div>
    </div>

    <script>
        function acceptCookies() {
            
            Cookies.set('showCookieBanner', 'false', { expires: 7, path: '/' });
            Cookies.set('cookiesAccepted', 'true', {expires: 7, path: '/'});
            toggleCookieBanner();
        }

        function rejectCookies() {
            Cookies.set('showCookieBanner', 'false', { expires: 7, path: '/' });
            toggleCookieBanner();
        }

        function toggleCookieBanner() {
            var showBanner = Cookies.get('showCookieBanner');
            var banner = document.getElementById('cookieBanner');

            if (showBanner === 'false') {
                banner.setAttribute('style', 'display: none;');
            } else {
                banner.setAttribute('style', 'display: block;');
            }
        }

        toggleCookieBanner();
    </script>
</div>

        
        <nav
    class="navbar is-primary "
    x-data="{ openNav: false }"
>
    <div class="container">
        <div class="navbar-brand">
            <a href="/" class="navbar-item">
                MVPavan's Notes
            </a>
            <a
                role="button"
                class="navbar-burger burger"
                aria-label="menu"
                aria-expanded="false"
                data-target="navMenu"
                :class="{ 'is-active': openNav }"
                x-on:click="openNav = !openNav"
            >
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu" :class="{ 'is-active': openNav }">
            <div class="navbar-start">
                <a href="/" class="navbar-item ">Home</a>
                
                    
                        
                            <a
                                href="/docs/"
                                class="navbar-item "
                            >Docs</a>
                        
                    
                
            </div>

            <div class="navbar-end">
                
            </div>
        </div>
    </div>
</nav>

        
            <section
    class="hero  is-medium  is-bold is-primary"
    
>
    <div class="hero-body ">
        <div class="container">
            <h1 class="title is-2"></h1>
            <p class="subtitle is-3"></p>
            
        </div>
    </div>
</section>

        
        

        <section class="section">
            <div class="container">
                <div class="columns is-multiline">
                    
                    <div class="column is-8">
                        

                        

                        

                        

                        
<div class="content">
    <ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#Resources</td>
          <td>Resources]]</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#Data Parallel</td>
          <td>Data Parallel]]</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#Deepspeed ZeRO Data Parallel</td>
          <td>Deepspeed ZeRO Data Parallel]]</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#Pipeline Parallel (Model Parallel)</td>
          <td>Pipeline Parallel (Model Parallel)]]</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#Tensor Parallel</td>
          <td>Tensor Parallel]]</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[[#2D &amp; 3D Parallelism</td>
          <td>2D &amp; 3D Parallelism]]</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h2 id="resources">Resources</h2>
<ul>
  <li>https://hanlab.mit.edu/courses/2023-fall-65940 lectures 17,18.</li>
  <li><a href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many">Fully Sharded Data Parallel (huggingface.co)</a></li>
</ul>

<p>Types of distributed training:</p>
<ul>
  <li><a href="#Data%20Parallel">Data Parallel:</a>: Replicate model across gpu and distribute data across each gpu.
    <ul>
      <li>DP, DDP: User if model fits in single gpu</li>
      <li><a href="#Deepspeed%20ZeRO%20Data%20Parallel">Deepspeed ZeRO Data Parallel:</a>:
        <ul>
          <li>Zero - 1, 2, 3(=FSDP in pytorch)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#Pipeline%20Parallel%20(Model%20Parallel)">Pipeline Parallel (Model Parallel):</a></li>
  <li><a href="#Tensor%20Parallel">Tensor Parallel:</a></li>
  <li><a href="#2D%20&amp;%203D%20Parallelism">2D &amp; 3D Parallelism:</a></li>
</ul>

<p>![[Pasted image 20240703184727.png]]</p>

<h2 id="data-parallel">Data Parallel</h2>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>DataParallel (DP)</th>
      <th>DistributedDataParallel (DDP)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Process Model</strong></td>
      <td>Single process, multiple threads</td>
      <td>Multiple processes, each handling one or more GPUs</td>
    </tr>
    <tr>
      <td><strong>Model Replication</strong></td>
      <td>Replicated on each GPU at each forward pass</td>
      <td>Replicated once per process</td>
    </tr>
    <tr>
      <td><strong>Input Data Handling</strong></td>
      <td>Splits input data across GPUs</td>
      <td>Splits input data across processes</td>
    </tr>
    <tr>
      <td><strong>Gradient Aggregation</strong></td>
      <td>Gradients averaged on the CPU/Single GPU</td>
      <td>Gradients synchronized across processes using NCCL</td>
    </tr>
    <tr>
      <td><strong>Performance</strong></td>
      <td>Better for smaller models.</td>
      <td>More efficient, better scaling across multiple GPUs and nodes.</td>
    </tr>
    <tr>
      <td><strong>Scalability</strong></td>
      <td>Best for single-node, multi-GPU setups</td>
      <td>Scales well across multiple nodes and GPUs</td>
    </tr>
    <tr>
      <td><strong>Synchronization</strong></td>
      <td>Implicit, handled by the framework</td>
      <td>Explicit, requires setting up distributed process groups</td>
    </tr>
    <tr>
      <td><strong>Code Example</strong></td>
      <td><code class="language-plaintext highlighter-rouge">model = nn.DataParallel(model).cuda()</code></td>
      <td><code class="language-plaintext highlighter-rouge">dist.init_process_group(backend='nccl'); model = DDP(model, device_ids=[local_rank])</code></td>
    </tr>
  </tbody>
</table>

<p>![[Pasted image 20240703173906.png]]</p>

<p>![[Pasted image 20240703171020.png]]</p>

<p>![[Pasted image 20240703174204.png]]</p>

<p>Conclusion:</p>
<ul>
  <li>The only communication DDP performs per batch is sending gradients, whereas DP does 5 different data exchanges per batch.</li>
  <li>Under DP gpu 0 performs a lot more work than the rest of the gpus, thus resulting in under-utilization of gpus.</li>
</ul>

<blockquote>
  <p>By default pytorch recommends DDP over DP, even for single node, multi gpu setup due to python GIL restrictions over multi threading.</p>
</blockquote>

<p>^82e76e</p>

<h2 id="deepspeed-zero-data-parallel">Deepspeed ZeRO Data Parallel</h2>
<p><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO &amp; DeepSpeed: New system optimizations enable training models with over 100 billion parameters - Microsoft Research</a></p>

<p>Core deep learning algorithm involves three components for any model apart from inputs:</p>
<ol>
  <li>Parameters</li>
  <li>Gradient</li>
  <li>Optimizer state
Thus Deepspeed zero is divided into three stages:
    <ul>
      <li>Stage 1 - Optimizer State Partitioning (Pos), sharding optimizer states across gpus.</li>
      <li>Stage 2 - Add Gradient Partitioning (Pos+g), sharding Gradients across gpus.</li>
      <li>Stage 3 - Add Parameter Partitioning (Pos+g+p), sharding Parameters across gpus.</li>
    </ul>
  </li>
</ol>

<p>![[Pasted image 20240703171554.png]]</p>

<p>Speed vs Memory:</p>
<ul>
  <li>speed - Zero 1 &gt;  Zero 2  &gt;  Zero 3</li>
  <li>Memory usage - Zero 1 &gt;  Zero 2 &gt;  Zero 3</li>
  <li>Communication Overhead - Zero 3 &gt;  Zero 2 &gt;  Zero 1</li>
  <li>Bandwidth requirement - Zero 3 &gt;  Zero 2 &gt;  Zero 1</li>
</ul>

<h2 id="pipeline-parallel-model-parallel">Pipeline Parallel (Model Parallel)</h2>

<p>Splits the model across gpus, useful if model size larger than single gpu memory.</p>

<p>![[Pasted image 20240703183650.png]]</p>

<p>![[Pasted image 20240703183721.png]]</p>

<ul>
  <li>Good for loading very large models</li>
  <li>Higher GPU idle time, it can be reduced using micro batches as shown above, still GPU utilization is poor compared to other techniques.</li>
</ul>

<h2 id="tensor-parallel">Tensor Parallel</h2>

<p>Split a weight tensor into N chunks, parallelize computation and aggregate results via all reduce.</p>

<p>![[Pasted image 20240703184517.png]]</p>

<h2 id="2d--3d-parallelism">2D &amp; 3D Parallelism</h2>

<p>![[Pasted image 20240703184944.png]]
![[Pasted image 20240703185009.png]]
![[Pasted image 20240703185030.png]]</p>

<p>There are libraries which support 3D parallelism out of the box, below are a few.</p>
<ul>
  <li>Deepspeed</li>
  <li>Nvidia Nemo.</li>
</ul>


</div>

                    </div>
                    
                        <div class="column is-4-desktop is-4-tablet">
                            <p class="title is-4">Latest Posts</p>

<div class="columns is-multiline">
    
</div>

                        </div>
                    
                </div>
            </div>
        </section>
        
            <footer class="footer">
    <div class="container">
        
            <div class="columns is-multiline">
                
            </div>
        

        <div class="content is-small has-text-centered">
            <p class="">Theme built by <a href="https://www.csrhymes.com">C.S. Rhymes</a></p>
        </div>
    </div>
</footer>

        
        <script src="/assets/js/app.js" type="text/javascript"></script><!-- footer scripts -->
</body>
</html>
